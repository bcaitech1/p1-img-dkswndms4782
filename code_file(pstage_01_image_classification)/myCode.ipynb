{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "embedded-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torchvision.models import resnext50_32x4d\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boolean-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험의 Randomness를 제거하여 실험이 같은 조건일 때 동일한 결과를 얻게 해줍니다.\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    print(f'이 실험은 seed {seed}로 고정되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 실험은 seed 2021로 고정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "class conf:\n",
    "    seed = 2021\n",
    "    data_dir = '/opt/ml/input/data/train'\n",
    "    model_dir = '/opt/ml/pstage_01_image_classification/model'\n",
    "    n_fold = 5\n",
    "\n",
    "seed_everything(conf.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-access",
   "metadata": {},
   "source": [
    "# 1. Dataset 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supported-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_label(image_name):\n",
    "    \"\"\"\n",
    "    이미지 파일 이름을 통해 mask label을 구합니다.\n",
    "\n",
    "    :param image_name: 학습 이미지 파일 이름\n",
    "    :return: mask label\n",
    "    \"\"\"\n",
    "    if 'incorrect_mask' in image_name:\n",
    "        return 1\n",
    "    elif 'normal' in image_name:\n",
    "        return 2\n",
    "    elif 'mask' in image_name:\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(f'No class for {image_name}')\n",
    "\n",
    "\n",
    "def get_gender_label(gender):\n",
    "    \"\"\"\n",
    "    gender label을 구하는 함수입니다.\n",
    "    :param gender: `male` or `female`\n",
    "    :return: gender label\n",
    "    \"\"\"\n",
    "    return 0 if gender == 'male' else 1\n",
    "\n",
    "\n",
    "def get_age_label(age):\n",
    "    \"\"\"\n",
    "    age label을 구하는 함수입니다.\n",
    "    :param age: 나이를 나타내는 int.\n",
    "    :return: age label\n",
    "    \"\"\"\n",
    "    return 0 if int(age) < 30 else 1 if int(age) < 60 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indonesian-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_gender_age(gender, age):\n",
    "    \"\"\"\n",
    "    gender와 age label을 조합하여 고유한 레이블을 만듭니다.\n",
    "    이를 구하는 이유는 train/val의 성별 및 연령 분포를 맞추기 위함입니다. (by Stratified K-Fold)\n",
    "    :param gender: `male` or `female`\n",
    "    :param age: 나이를 나타내는 int.\n",
    "    :return: gender & age label을 조합한 레이블\n",
    "    \"\"\"\n",
    "    gender_label = get_gender_label(gender)\n",
    "    age_label = get_age_label(age)\n",
    "    return gender_label * 3 + age_label\n",
    "\n",
    "\n",
    "def convert_label(image_path, sep=False):\n",
    "    \"\"\"\n",
    "    이미지의 label을 구하는 함수입니다.\n",
    "    :param image_path: 이미지 경로를 나타내는 str\n",
    "    :param sep: 마스크, 성별, 연령 label을 따로 반환할건지 합쳐서 할지 나타내는 bool 인수입니다. 참일 경우 따로 반환합니다.\n",
    "    :return: 이미지의 label (int or list)\n",
    "    \"\"\"\n",
    "    image_name = image_path.split('/')[-1]\n",
    "    mask_label = get_mask_label(image_name)\n",
    "\n",
    "    profile = image_path.split('/')[-2]\n",
    "    image_id, gender, race, age = profile.split(\"_\")\n",
    "    gender_label = get_gender_label(gender)\n",
    "    age_label = get_age_label(age)\n",
    "    if sep:\n",
    "        return mask_label, gender_label, age_label\n",
    "    else:\n",
    "        return mask_label * 6 + gender_label * 3 + age_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "skilled-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    \".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\",\n",
    "    \".PNG\", \".ppm\", \".PPM\", \".bmp\", \".BMP\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filepath):\n",
    "    \"\"\"\n",
    "    해당 파일이 이미지 파일인지 확인합니다.\n",
    "    \"\"\"\n",
    "    return any(filepath.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def remove_hidden_file(filepath):\n",
    "    \"\"\"\n",
    "    `._`로 시작하는 숨김 파일일 경우 False를 반환합니다.\n",
    "    \"\"\"\n",
    "    filename = filepath.split('/')[-1]\n",
    "    return False if filename.startswith('._') else True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-germany",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sapphire-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384)):\n",
    "    \"\"\"\n",
    "    Augmentation 함수를 반홥합니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            CenterCrop(448, 336, p=1.0),\n",
    "            RandomResizedCrop(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.3),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.3),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.3),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.3),\n",
    "            Cutout(p=0.3),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            CenterCrop(448, 336, p=1.0),\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "changed-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    \"\"\"\n",
    "    이미지를 불러옵니다.\n",
    "    \"\"\"\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb\n",
    "\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, info, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.info = info\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = [path for name in info.path.values for path in glob.glob(os.path.join(image_dir, name, '*'))]\n",
    "        self.image_paths = list(filter(is_image_file, self.image_paths))\n",
    "        self.image_paths = list(filter(remove_hidden_file, self.image_paths))\n",
    "\n",
    "        self.labels = [convert_label(path, sep=False) for path in self.image_paths]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = get_img(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        label = torch.eye(18)[label]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-tuner",
   "metadata": {},
   "source": [
    "# 3. 학습/검증 데이터셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "african-document",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = pd.read_csv(f'{conf.data_dir}/train.csv')\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "egyptian-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "info['gender_age'] = info.apply(lambda x: convert_gender_age(x.gender, x.age), axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=conf.n_fold, shuffle=True)\n",
    "info.loc[:, 'fold'] = 0\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(X=info.index, y=info.gender_age.values)):\n",
    "    info.loc[info.iloc[val_index].index, 'fold'] = fold_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-piece",
   "metadata": {},
   "source": [
    "# 4. Pytorch 데이터셋을 Fold를 활용하여 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "approximate-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = os.path.join(conf.data_dir, 'images')\n",
    "\n",
    "for fold_idx in range(conf.n_fold):\n",
    "    train = info[info.fold != fold_idx].reset_index(drop=True)\n",
    "    val = info[info.fold == fold_idx].reset_index(drop=True)\n",
    "\n",
    "    transforms = get_transforms()\n",
    "    train_dataset = MaskDataset(image_dir, train, transforms['train'])\n",
    "    val_dataset = MaskDataset(image_dir, val, transforms['val'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True,\n",
    "                              num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False,\n",
    "                            num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-highlight",
   "metadata": {},
   "source": [
    "# 5.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "historical-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "literary-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-weather",
   "metadata": {},
   "source": [
    "# 6.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "split-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "resNext = timm.create_model('resnext50_32x4d', True)\n",
    "n_features = resNext.fc.in_features\n",
    "resNext.fc = nn.Linear(n_features, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cultural-associate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "efficientNet = EfficientNet.from_pretrained('efficientnet-b0', num_classes = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-acting",
   "metadata": {},
   "source": [
    "# 7.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "premier-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from loss import create_criterion\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "impossible-boxing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def increment_path(path, exist_ok=False):\n",
    "    path = Path(path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "voluntary-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "pending-department",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/1](20/3780) || training loss 1.819 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](40/3780) || training loss 2.049 || training accuracy 1.25% || lr 0.001\n",
      "Epoch[0/1](60/3780) || training loss 2.005 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](80/3780) || training loss 2.122 || training accuracy 1.02% || lr 0.001\n",
      "Epoch[0/1](100/3780) || training loss 1.906 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](120/3780) || training loss 1.809 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](140/3780) || training loss 1.822 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](160/3780) || training loss 1.851 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](180/3780) || training loss 1.793 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](200/3780) || training loss 1.814 || training accuracy 1.17% || lr 0.001\n",
      "Epoch[0/1](220/3780) || training loss 1.636 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](240/3780) || training loss 2.123 || training accuracy 1.02% || lr 0.001\n",
      "Epoch[0/1](260/3780) || training loss 1.742 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](280/3780) || training loss 1.551 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](300/3780) || training loss 2.087 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](320/3780) || training loss 1.86 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](340/3780) || training loss 1.851 || training accuracy 1.17% || lr 0.001\n",
      "Epoch[0/1](360/3780) || training loss 1.978 || training accuracy 1.33% || lr 0.001\n",
      "Epoch[0/1](380/3780) || training loss 1.936 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](400/3780) || training loss 1.885 || training accuracy 1.25% || lr 0.001\n",
      "Epoch[0/1](420/3780) || training loss 1.917 || training accuracy 1.17% || lr 0.001\n",
      "Epoch[0/1](440/3780) || training loss 1.829 || training accuracy 1.17% || lr 0.001\n",
      "Epoch[0/1](460/3780) || training loss 1.727 || training accuracy 1.33% || lr 0.001\n",
      "Epoch[0/1](480/3780) || training loss 2.02 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](500/3780) || training loss 1.756 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](520/3780) || training loss 1.777 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](540/3780) || training loss 1.848 || training accuracy 1.09% || lr 0.001\n",
      "Epoch[0/1](560/3780) || training loss 1.829 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](580/3780) || training loss 1.815 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](600/3780) || training loss 1.756 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](620/3780) || training loss 1.814 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](640/3780) || training loss 1.75 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](660/3780) || training loss 1.798 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](680/3780) || training loss 1.741 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](700/3780) || training loss 1.84 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](720/3780) || training loss 1.89 || training accuracy 1.25% || lr 0.001\n",
      "Epoch[0/1](740/3780) || training loss 1.611 || training accuracy 1.33% || lr 0.001\n",
      "Epoch[0/1](760/3780) || training loss 1.896 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](780/3780) || training loss 1.874 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](800/3780) || training loss 1.841 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](820/3780) || training loss 1.894 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](840/3780) || training loss 1.697 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](860/3780) || training loss 1.651 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](880/3780) || training loss 1.895 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](900/3780) || training loss 1.838 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](920/3780) || training loss 1.798 || training accuracy 1.25% || lr 0.001\n",
      "Epoch[0/1](940/3780) || training loss 1.682 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](960/3780) || training loss 1.805 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](980/3780) || training loss 1.896 || training accuracy 1.33% || lr 0.001\n",
      "Epoch[0/1](1000/3780) || training loss 1.835 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](1020/3780) || training loss 1.485 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1040/3780) || training loss 1.666 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1060/3780) || training loss 1.829 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1080/3780) || training loss 1.834 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1100/3780) || training loss 1.689 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](1120/3780) || training loss 1.705 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](1140/3780) || training loss 1.758 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](1160/3780) || training loss 1.582 || training accuracy 2.34% || lr 0.001\n",
      "Epoch[0/1](1180/3780) || training loss 1.792 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](1200/3780) || training loss 1.578 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](1220/3780) || training loss 1.834 || training accuracy 1.33% || lr 0.001\n",
      "Epoch[0/1](1240/3780) || training loss 1.958 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](1260/3780) || training loss 1.787 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1280/3780) || training loss 1.759 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](1300/3780) || training loss 1.581 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](1320/3780) || training loss 1.804 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1340/3780) || training loss 1.783 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](1360/3780) || training loss 1.705 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](1380/3780) || training loss 1.711 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1400/3780) || training loss 1.488 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](1420/3780) || training loss 1.716 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](1440/3780) || training loss 1.685 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](1460/3780) || training loss 1.39 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](1480/3780) || training loss 1.59 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](1500/3780) || training loss 1.471 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](1520/3780) || training loss 1.639 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](1540/3780) || training loss 1.688 || training accuracy 0.94% || lr 0.001\n",
      "Epoch[0/1](1560/3780) || training loss 1.563 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](1580/3780) || training loss 1.537 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](1600/3780) || training loss 1.635 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1620/3780) || training loss 1.749 || training accuracy 1.41% || lr 0.001\n",
      "Epoch[0/1](1640/3780) || training loss 1.536 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](1660/3780) || training loss 1.677 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](1680/3780) || training loss 1.619 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](1700/3780) || training loss 1.739 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1720/3780) || training loss 1.532 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](1740/3780) || training loss 1.858 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1760/3780) || training loss 1.605 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1780/3780) || training loss 1.634 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](1800/3780) || training loss 1.488 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](1820/3780) || training loss 1.619 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](1840/3780) || training loss 1.761 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1860/3780) || training loss 1.812 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](1880/3780) || training loss 1.785 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](1900/3780) || training loss 1.73 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](1920/3780) || training loss 1.487 || training accuracy 2.50% || lr 0.001\n",
      "Epoch[0/1](1940/3780) || training loss 1.641 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](1960/3780) || training loss 1.648 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](1980/3780) || training loss 1.596 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](2000/3780) || training loss 1.574 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](2020/3780) || training loss 1.603 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](2040/3780) || training loss 1.584 || training accuracy 2.58% || lr 0.001\n",
      "Epoch[0/1](2060/3780) || training loss 1.48 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](2080/3780) || training loss 1.813 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](2100/3780) || training loss 1.609 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](2120/3780) || training loss 1.586 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2140/3780) || training loss 1.576 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](2160/3780) || training loss 1.677 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](2180/3780) || training loss 1.716 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](2200/3780) || training loss 1.631 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](2220/3780) || training loss 1.548 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](2240/3780) || training loss 1.426 || training accuracy 2.89% || lr 0.001\n",
      "Epoch[0/1](2260/3780) || training loss 1.536 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](2280/3780) || training loss 1.48 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2300/3780) || training loss 1.572 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](2320/3780) || training loss 1.767 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](2340/3780) || training loss 1.623 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](2360/3780) || training loss 1.699 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](2380/3780) || training loss 1.367 || training accuracy 2.34% || lr 0.001\n",
      "Epoch[0/1](2400/3780) || training loss 1.702 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](2420/3780) || training loss 1.39 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](2440/3780) || training loss 1.499 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](2460/3780) || training loss 1.578 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](2480/3780) || training loss 1.478 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](2500/3780) || training loss 1.637 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](2520/3780) || training loss 1.634 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](2540/3780) || training loss 1.77 || training accuracy 1.56% || lr 0.001\n",
      "Epoch[0/1](2560/3780) || training loss 1.576 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2580/3780) || training loss 1.484 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2600/3780) || training loss 1.426 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](2620/3780) || training loss 1.665 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2640/3780) || training loss 1.687 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](2660/3780) || training loss 1.564 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](2680/3780) || training loss 1.617 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](2700/3780) || training loss 1.454 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](2720/3780) || training loss 1.343 || training accuracy 2.66% || lr 0.001\n",
      "Epoch[0/1](2740/3780) || training loss 1.457 || training accuracy 2.58% || lr 0.001\n",
      "Epoch[0/1](2760/3780) || training loss 1.658 || training accuracy 2.11% || lr 0.001\n",
      "Epoch[0/1](2780/3780) || training loss 1.526 || training accuracy 2.50% || lr 0.001\n",
      "Epoch[0/1](2800/3780) || training loss 1.448 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](2820/3780) || training loss 1.486 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](2840/3780) || training loss 1.725 || training accuracy 1.64% || lr 0.001\n",
      "Epoch[0/1](2860/3780) || training loss 1.517 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](2880/3780) || training loss 1.617 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](2900/3780) || training loss 1.53 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](2920/3780) || training loss 1.523 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](2940/3780) || training loss 1.642 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](2960/3780) || training loss 1.335 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](2980/3780) || training loss 1.374 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3000/3780) || training loss 1.486 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](3020/3780) || training loss 1.266 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](3040/3780) || training loss 1.515 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](3060/3780) || training loss 1.668 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](3080/3780) || training loss 1.34 || training accuracy 2.50% || lr 0.001\n",
      "Epoch[0/1](3100/3780) || training loss 1.537 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3120/3780) || training loss 1.468 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3140/3780) || training loss 1.302 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](3160/3780) || training loss 1.557 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](3180/3780) || training loss 1.636 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3200/3780) || training loss 1.68 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](3220/3780) || training loss 1.585 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](3240/3780) || training loss 1.548 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3260/3780) || training loss 1.415 || training accuracy 3.12% || lr 0.001\n",
      "Epoch[0/1](3280/3780) || training loss 1.61 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3300/3780) || training loss 1.329 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3320/3780) || training loss 1.851 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](3340/3780) || training loss 1.595 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](3360/3780) || training loss 1.365 || training accuracy 2.58% || lr 0.001\n",
      "Epoch[0/1](3380/3780) || training loss 1.675 || training accuracy 2.34% || lr 0.001\n",
      "Epoch[0/1](3400/3780) || training loss 1.323 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3420/3780) || training loss 1.558 || training accuracy 1.80% || lr 0.001\n",
      "Epoch[0/1](3440/3780) || training loss 1.275 || training accuracy 2.34% || lr 0.001\n",
      "Epoch[0/1](3460/3780) || training loss 1.329 || training accuracy 2.66% || lr 0.001\n",
      "Epoch[0/1](3480/3780) || training loss 1.344 || training accuracy 2.42% || lr 0.001\n",
      "Epoch[0/1](3500/3780) || training loss 1.253 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](3520/3780) || training loss 1.528 || training accuracy 1.72% || lr 0.001\n",
      "Epoch[0/1](3540/3780) || training loss 1.658 || training accuracy 1.88% || lr 0.001\n",
      "Epoch[0/1](3560/3780) || training loss 1.591 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3580/3780) || training loss 1.507 || training accuracy 2.89% || lr 0.001\n",
      "Epoch[0/1](3600/3780) || training loss 1.475 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3620/3780) || training loss 1.471 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](3640/3780) || training loss 1.506 || training accuracy 2.27% || lr 0.001\n",
      "Epoch[0/1](3660/3780) || training loss 1.612 || training accuracy 1.48% || lr 0.001\n",
      "Epoch[0/1](3680/3780) || training loss 1.623 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](3700/3780) || training loss 1.502 || training accuracy 1.95% || lr 0.001\n",
      "Epoch[0/1](3720/3780) || training loss 1.344 || training accuracy 2.50% || lr 0.001\n",
      "Epoch[0/1](3740/3780) || training loss 1.541 || training accuracy 2.03% || lr 0.001\n",
      "Epoch[0/1](3760/3780) || training loss 1.548 || training accuracy 2.19% || lr 0.001\n",
      "Epoch[0/1](3780/3780) || training loss 1.392 || training accuracy 2.34% || lr 0.001\n",
      "Calculating validation results...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-c6443098168b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mloss_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0macc_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mval_loss_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pstage_01_image_classification/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, target_tensor)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "from loss import create_criterion\n",
    "image_dir = os.path.join(conf.data_dir, 'images')\n",
    "save_dir = increment_path(os.path.join(conf.model_dir, 'mycode'))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "epochs = 1\n",
    "\n",
    "model = resNext.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    " # -- loss & metric\n",
    "criterion = create_criterion('focal') \n",
    "opt_module = torch.optim.Adam\n",
    "optimizer = opt_module(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "scheduler = StepLR(optimizer, 20, gamma=0.5)\n",
    "\n",
    "\n",
    "# -- logging\n",
    "logger = SummaryWriter(log_dir=save_dir)\n",
    "#with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "#    json.dump(var(args), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "for fold_idx in range(conf.n_fold):\n",
    "    \n",
    "    train = info[info.fold != fold_idx].reset_index(drop=True)\n",
    "    val = info[info.fold == fold_idx].reset_index(drop=True)\n",
    "\n",
    "    transforms = get_transforms()\n",
    "    train_dataset = MaskDataset(image_dir, train, transforms['train'])\n",
    "    val_dataset = MaskDataset(image_dir, val, transforms['val'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n",
    "                              num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,\n",
    "                            num_workers=3)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #####################################\n",
    "            outs = model(inputs).to(device).float()\n",
    "            preds = torch.argmax(outs, dim=-1)            \n",
    "\n",
    "            loss = criterion(outs, torch.max(labels,1)[1])\n",
    "            ##########이거 에러 정말 해결하기 힘드네^^,,,\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            labels = torch.argmax(labels, dim = -1)\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % 20 == 0:\n",
    "                train_loss = loss_value / 20\n",
    "                train_acc = matches / 64 /20\n",
    "                current_lr = get_lr(optimizer)\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "                logger.add_scalar(\"Train/loss\", train_loss, epoch * len(train_loader) + idx)\n",
    "                logger.add_scalar(\"Train/accuracy\", train_acc, epoch * len(train_loader) + idx)\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            figure = None\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outs = model(inputs)\n",
    "                \n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "                if figure is None:\n",
    "                    inputs_np = torch.clone(inputs).detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "                    inputs_np = dataset_module.denormalize_image(inputs_np, dataset.mean, dataset.std)\n",
    "                    figure = grid_image(inputs_np, labels, preds) #  dataset != \"MaskSplitByProfileDataset\"\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "            best_val_loss = min(best_val_loss, val_loss)\n",
    "            if val_acc > best_val_acc:\n",
    "                print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "                torch.save(model.module.state_dict(), f\"{save_dir}/best.pth\")\n",
    "                best_val_acc = val_acc\n",
    "            torch.save(model.module.state_dict(), f\"{save_dir}/last.pth\")\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )\n",
    "            logger.add_scalar(\"Val/loss\", val_loss, epoch)\n",
    "            logger.add_scalar(\"Val/accuracy\", val_acc, epoch)\n",
    "            logger.add_figure(\"results\", figure, epoch)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-dynamics",
   "metadata": {},
   "source": [
    "# 8.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resNext\n",
    "model_path = os.path.join(save_dir, 'best.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "img_root = os.path.join(data_dir, 'images')\n",
    "info_path = os.path.join(data_dir, 'info.csv')\n",
    "info = pd.read_csv(info_path)\n",
    "\n",
    "img_paths = [os.path.join(img_root, img_id) for img_id in info.ImageID]\n",
    "dataset = TestDataset(img_paths,(96, 128))\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=use_cuda,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"Calculating inference results..\")\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for idx, images in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "\n",
    "info['ans'] = preds\n",
    "info.to_csv(os.path.join(output_dir, f'output.csv'), index=False)\n",
    "print(f'Inference Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-structure",
   "metadata": {},
   "source": [
    "# 9.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-trinidad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-joseph",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
